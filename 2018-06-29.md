## 2018-06-29

**《Towards High Performance Video Object Detection》**  

**CVPR 2018 SPOTLIGHT**  

**Abstract:** 最近几年图像目标检测已经有显著进步。然而视频目标检测却收到较少关注，尽管它更具挑战性且在实际情景中更重要。基于最近[37,36]的工作，本文提出一个统一的基于多帧end-to-end特征学习和跨帧移动原则的方法。我们的方法以三个新技术扩展了以前的工作，坚实的推进了性能上界(速度/准确度权衡)，直向高性能视频目标检测。  

**Note:**  

**Sparse Feature Propagation:** Feature maps on non-key frame i are propagated from its preceding key frame k using frame pixel-wise motion of a two dimensional motion field Mi->k, that denotes as Fk->i = W(Fk, Mi->k). Detection network works on Fi-k instead of Fi computing from feature extractor network. Motion field Mi->k is estimated by a lightweight flow network Nflow(Ik, Ii) = Mi->k, which takes two frames Ik, Ii as input. So it is fast because of the lightweight network Nflow.  

**Dense Feature Aggregation:** For any frame i, feature maps of all frames within a temporal window [i-r, i+r] are warped onto frame i in the same way as Fj->i = W(Fk, Mi-j) (j in [i-r, i+r]). Juse like every frame is a key frame. Fi at frame i is obtained as the weighted average of all such feature maps. The weight is adaptively computed as the similarity between the propagated feature maps Fk->i and the real feature maps Fi calculated by feature extractor.  

**Sparsely Recursive Feature Aggregation:** For two key frames k and k', aggregated feature at frame k' is Fk' = Wk->k'×Fk->k' + Wk'->k'×Fk'. This is a recursive vesion of Dense Feature Aggregation, and Fk->k' = W(Fk, Mk'->k) as Sparse Feature Progation, and × denotes element-wise multiplication, and weight is correspondingly normalized  by Wk->k' + Wk'->k- = 1.  

**Spatially-adaptive Partial Feature Updating:** To quantify whether the propagated feature Fk->i is a good approximation of Fi, a feature temporal consistency Qk->i is introduced, then together with motion field Mi-k, we have {Mi-k, Qi-k} = Nflow(Ik, Ii). If Qk-i(p) less equal t, the propagated feature Fk->p(p) is inconsisent with the real feature Fi(p), that means Fk-i(p) is a bad approximation and we shall update with real feature Fi(p). So we have Fi = Uk-i×Nfeat(Ii) + (1-Uk-i)×Fk-i, where Uk->i = 1 if Qk-i(p) less equal t, and Uk-i(p) = 0 otherwise. Only Update partial feature maps.  

**Temporally-adaptive Key Frame Scheduling:** If Qk-i(p) less equal t for all position on the image, or most position, we shall mark this frame as a new key frame. Generally, we can denote key = is_key(Qk-i) = sum(1 if (Qk-i(p) less equal t) else 0)/Np as a indicator. If key > threshold, there is a new key frame and we shall recompute feature maps. This scheduling policy is adaptive to the varying dynamics in temporal domain, not like a naive scheduling policy as picking a key frame at a prefixed rate.  

![](./.assets/Towards_High_Performance_Video_Object_Detection_Figure_2.png)
![](./.assets/Towards_High_Performance_Video_Object_Detection_Algorithm_1.png)

**Framework:**  

**Flow network:** FlowNet  
**Feature network:** ResNet-101  
**Detection netork:** R-FCN  

**Link:** https://arxiv.org/abs/1711.11577  

---
**《Cascade R-CNN: Delving into High Quality Object Detection》**  

**CVPR 2018 SPOTLIGHT**  

**Abstract:** 在目标检测中，需要交集并集比(intersection over union -- IOU)临界值来定义正样本和负样本。一个目标检测器，以低IOU如0.5训练，通常产生噪声检测。然而，随着IOU临界值增加，检测器性能倾向于退化。两个主要因素对此负责：(1)训练过程中的过拟合，源于指数级消失的正样本，以及(2)在优化检测器的IOU和输入假设的IOU之间的推测时间不匹配。一个多阶段目标检测架构，Cascade R-CNN，被提出用于解决这些问题。它包含一系列用不断增加的IOU临界值训练的检测器，对于最近错误正样本更有序列选择性。检测器逐个阶段进行训练，利用一个检测器的输出是训练下一个更高质量检测器的好的分布这个现象。逐渐改进假设的重抽样保证所有检测器有相同尺寸样本的正集合，减少过拟合问题。相同层叠步骤被应用与推测，能够在假设和每个阶段检测器质量之间更近的匹配。一个简单的Cascade R-CNN实现展示了在具有挑战性的COCO数据集上超越所有单模型目标检测器。实验也显示Cascade R-CNN跨越各种检测器架构具有广泛适用性，始终对于各自基准检测器强度获得收益。代码地址：[https://github.com/zhaoweicai/cascade-rcnn](https://github.com/zhaoweicai/cascade-rcnn)。  

**Note:**

**Framework:**  

**Link:** https://arxiv.org/abs/1712.00726  
